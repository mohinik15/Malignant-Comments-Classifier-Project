{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd8f6fdc",
   "metadata": {},
   "source": [
    "# MALIGNANT COMMENTS CLASSIFICATION\n",
    "\n",
    "Problem Statement\n",
    "The proliferation of social media enables people to express their opinions widely online. However, at the same time, this has resulted in the emergence of conflict and hate, making online environments uninviting for users. Although researchers have found that hate is a problem across multiple platforms, there is a lack of models for online hate detection. Online hate, described as abusive language, aggression, cyberbullying, hatefulness and many others has been identified as a major threat on online social media platforms. Social media platforms are the most prominent grounds for such toxic behaviour.\n",
    "There has been a remarkable increase in the cases of cyberbullying and trolls on various social media platforms. Many celebrities and influences are facing backlashes from people and have to come across hateful and offensive comments. This can take a toll on anyone and affect them mentally leading to depression, mental illness, self-hatred and suicidal thoughts.\n",
    "Internet comments are bastions of hatred and vitriol. While online anonymity has provided a new outlet for aggression and hate speech, machine learning can be used to fight it. The problem we sought to solve was the tagging of internet comments that are aggressive towards other users. This means that insults to third parties such as celebrities will be tagged as unoffensive, but “u are an idiot” is clearly offensive. Our goal is to build a prototype of online hate and abuse comment classifier which can used to classify hate and offensive comments so that it can be controlled and restricted from spreading hatred and cyberbullying.\n",
    "\n",
    "Data Set Description\n",
    "The data set contains the training set, which has approximately 1,59,000 samples and the test set which contains nearly 1,53,000 samples. All the data samples contain 8 fields which includes ‘Id’, ‘Comments’, ‘Malignant’, ‘Highly malignant’, ‘Rude’, ‘Threat’, ‘Abuse’ and ‘Loathe’. The label can be either 0 or 1, where 0 denotes a NO while 1 denotes a YES. There are various comments which have multiple labels. The first attribute is a unique ID associated with each comment.\n",
    "The data set includes:\n",
    "\n",
    "Malignant: It is the Label column, which includes values 0 and 1, denoting if the comment is malignant or not.\n",
    "Highly Malignant: It denotes comments that are highly malignant and hurtful.\n",
    "Rude: It denotes comments that are very rude and offensive.\n",
    "Threat: It contains indication of the comments that are giving any threat to someone.\n",
    "Abuse: It is for comments that are abusive in nature.\n",
    "Loathe: It describes the comments which are hateful and loathing in nature.\n",
    "ID: It includes unique Ids associated with each comment text given.\n",
    "Comment text: This column contains the comments extracted from various social media platforms.\n",
    "This project is more about exploration, feature engineering and classification that can be done on this data. Since the data set is huge and includes many categories of comments, we can do good amount of data exploration and derive some interesting features using the comments text column available. You need to build a model that can differentiate between comments and its categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import missingno\n",
    "import pandas_profiling\n",
    "from scipy import interp\n",
    "import scikitplot as skplt\n",
    "from itertools import cycle\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize, regexp_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import timeit, sys\n",
    "from sklearn import metrics\n",
    "import tqdm.notebook as tqdm\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import hamming_loss, log_loss, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, multilabel_confusion_matrix\n",
    "from scikitplot.metrics import plot_roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2060229",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcee9dc",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have {} Rows and {} Columns in our dataframe\".format(df_train.shape[0], df_train.shape[1]))\n",
    "df_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5926b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "We can see that our training dataframe consists of 159571 rows and 8 columns in total.\n",
    "\n",
    "Column Description:\n",
    "\n",
    "id : A unique id aligned with each comment text.\n",
    "comment_text : It includes the comment text.\n",
    "malignant : It is a column with binary values depicting which comments are malignant in nature.\n",
    "highly_malignant : Binary column with labels for highly malignant text.\n",
    "rude : Binary column with labels for comments that are rude in nature.\n",
    "threat : Binary column with labels for threatening context in the comments.\n",
    "abuse : Binary column with labels with abusive behaviour.\n",
    "loathe : Label to comments that are full of loathe and hatred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isna().sum() # checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc12b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "missingno.bar(df_train, figsize = (25,5), color=\"tab:red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460aab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking ratio of data which contains malignant comments and normal or unoffensive comments.\n",
    "output_labels = df_train.columns[2:]\n",
    "\n",
    "# counting non-zero rows i.e. Malignant Comments\n",
    "malignant_comments = len(df_train[df_train[output_labels].any(axis=1)])\n",
    "\n",
    "# counting rows containing zero i.e. Normal Comments\n",
    "normal_comments = len(df_train)-malignant_comments\n",
    "\n",
    "print(f\"Total Malignant Comments: {malignant_comments} ({round(malignant_comments*100/len(df_train),2)}%)\")\n",
    "print(f\"Total Normal Comments: {normal_comments} ({round(normal_comments*100/len(df_train),2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a71e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the length of comments and storing it into another column 'original_length'\n",
    "# copying df_train into another object df\n",
    "df = df_train.copy()\n",
    "df['original_length'] = df.comment_text.str.len()\n",
    "\n",
    "# checking the first five and last five rows here\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf38a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleansing\n",
    "\n",
    "# as the feature 'id' has no relevance w.r.t. model training I am dropping this column\n",
    "df.drop(columns=['id'],inplace=True)\n",
    "# converting comment text to lowercase format\n",
    "df['comment_text'] = df.comment_text.str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1496231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing and Replacing unwanted characters in the comment_text column\n",
    "\n",
    "# Replacing '\\n' with ' '\n",
    "df.comment_text = df.comment_text.str.replace('\\n',' ')\n",
    "\n",
    "# Keeping only text with letters a to z, 0 to 9 and words like can't, don't, couldn't etc\n",
    "df.comment_text = df.comment_text.apply(lambda x: ' '.join(regexp_tokenize(x,\"[a-z']+\")))\n",
    "\n",
    "# Removing Stop Words and Punctuations\n",
    "\n",
    "# Getting the list of stop words of english language as set\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Updating the stop_words set by adding letters from a to z\n",
    "for ch in range(ord('a'),ord('z')+1):\n",
    "    stop_words.update(chr(ch))\n",
    "    \n",
    "# Updating stop_words further by adding some custom words\n",
    "custom_words = (\"d'aww\",\"mr\",\"hmm\",\"umm\",\"also\",\"maybe\",\"that's\",\"he's\",\"she's\",\"i'll\",\"he'll\",\"she'll\",\"us\",\n",
    "                \"ok\",\"there's\",\"hey\",\"heh\",\"hi\",\"oh\",\"bbq\",\"i'm\",\"i've\",\"nt\",\"can't\",\"could\",\"ur\",\"re\",\"ve\",\n",
    "                \"rofl\",\"lol\",\"stfu\",\"lmk\",\"ily\",\"yolo\",\"smh\",\"lmfao\",\"nvm\",\"ikr\",\"ofc\",\"omg\",\"ilu\")\n",
    "stop_words.update(custom_words)\n",
    "\n",
    "# Checking the new list of stop words\n",
    "print(\"New list of custom stop words are as follows:\\n\\n\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fe12f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words\n",
    "df.comment_text = df.comment_text.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words).strip())\n",
    "\n",
    "# Removing punctuations\n",
    "df.comment_text = df.comment_text.str.replace(\"[^\\w\\d\\s]\",\"\")\n",
    "\n",
    "# Checking any 10 random rows to see the applied changes\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a8e2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming words\n",
    "snb_stem = SnowballStemmer('english')\n",
    "df.comment_text = df.comment_text.apply(lambda x: ' '.join(snb_stem.stem(word) for word in word_tokenize(x)))\n",
    "\n",
    "# Checking any 10 random rows to see the applied changes\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37dc7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the length of comment_text after cleaning and storing it in cleaned_length variable\n",
    "df[\"cleaned_length\"] = df.comment_text.str.len()\n",
    "\n",
    "# Taking a loot at first 10 rows of data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ec17ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now checking the percentage of length cleaned\n",
    "print(f\"Total Original Length        : {df.original_length.sum()}\")\n",
    "print(f\"Total Cleaned Length         : {df.cleaned_length.sum()}\")\n",
    "print(f\"Percentage of Length Cleaned : {(df.original_length.sum()-df.cleaned_length.sum())*100/df.original_length.sum()}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f62f58",
   "metadata": {},
   "source": [
    "# visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing normal comments and bad comments using count plot\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "for i in range(2):\n",
    "    sns.countplot(data=df[output_labels][df[output_labels]==i], ax=ax[i])\n",
    "    if i == 0:\n",
    "        ax[i].set_title(\"Count Plot for Normal Comments\\n\")\n",
    "    else:\n",
    "        ax[i].set_title(\"Count Plot for Bad Comments\\n\")\n",
    "        \n",
    "    ax[i].set_xticklabels(output_labels, rotation=90, ha=\"right\")\n",
    "    p=0\n",
    "    for prop in ax[i].patches:\n",
    "        count = prop.get_height()\n",
    "        s = f\"{count} ({round(count*100/len(df),2)}%)\"\n",
    "        ax[i].text(p,count/2,s,rotation=90, ha=\"center\", fontweight=\"bold\")\n",
    "        p += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b436e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the comment text length distribution before cleaning and after cleaning\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(15,6))\n",
    "j=0\n",
    "colors = ['orange','green']\n",
    "for i in df.columns[-2:]:\n",
    "    label_text = f\"Distribution of Comment Length: {i}\"\n",
    "    sns.distplot(df[i],ax=ax[j],bins=20,color=colors[j],label=label_text)\n",
    "    ax[j].set_xlabel(\"Message Length\")\n",
    "    ax[j].legend()\n",
    "    j += 1\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d3291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the label distribution of comments using pie chart\n",
    "\n",
    "comments_labels = ['malignant', 'highly_malignant', 'rude', 'threat', 'abuse', 'loathe']\n",
    "df_distribution = df_train[comments_labels].sum()\\\n",
    "                            .to_frame()\\\n",
    "                            .rename(columns={0: 'count'})\\\n",
    "                            .sort_values('count')\n",
    "\n",
    "df_distribution.plot.pie(y = 'count', title = 'Label distribution over comments', autopct='%.2f', figsize = (15, 10))\\\n",
    "                            .legend(loc='center left', bbox_to_anchor=(1.3, 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802a2dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting heatmap for visualizing the correlation\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "corr = df_train.corr() # corr() function provides the correlation value of each column\n",
    "sns.heatmap(corr, linewidth=0.5, linecolor='black', fmt='.0%', cmap='YlGn_r', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordCloud: Getting sense of loud words in each of the output labels.\n",
    "\n",
    "cols = 3\n",
    "rows = len(output_labels)//cols\n",
    "if len(output_labels) % cols != 0:\n",
    "    rows += 1\n",
    "    \n",
    "fig = plt.figure(figsize=(16,rows*cols*1.8))\n",
    "fig.subplots_adjust(top=0.8, hspace=0.3)\n",
    "\n",
    "p=1\n",
    "for i in output_labels:\n",
    "    word_cloud = WordCloud(height=650, width=800,\n",
    "                           background_color=\"white\",max_words=80).generate(' '.join(df.comment_text[df[i]==1]))\n",
    "    ax = fig.add_subplot(rows,cols,p)\n",
    "    ax.imshow(word_cloud)\n",
    "    ax.set_title(f\"WordCloud for {i} column\",fontsize=14)\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('r')\n",
    "       \n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    p += 1\n",
    "\n",
    "fig.suptitle(\"WordCloud: Representation of Loud words in BAD COMMENTS\",fontsize=16)\n",
    "fig.tight_layout(pad=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd428cf5",
   "metadata": {},
   "source": [
    "# Data Preparation for Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6928f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert text to Vectors\n",
    "\n",
    "# Converting text to vectors using TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=4000)\n",
    "features = tfidf.fit_transform(df.comment_text).toarray()\n",
    "\n",
    "# Checking the shape of features\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f69060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Seperating Input and Output Variables\n",
    "\n",
    "# input variables\n",
    "X = features\n",
    "\n",
    "# output variables\n",
    "Y = csr_matrix(df[output_labels]).toarray()\n",
    "\n",
    "# checking shapes of input and output variables to take care of data imbalance issue\n",
    "print(\"Input Variable Shape:\", X.shape)\n",
    "print(\"Output Variable Shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4f663",
   "metadata": {},
   "source": [
    "# Classification Machine Learning Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training and Testing Model on our train dataset\n",
    "\n",
    "# Creating a function to train and test model\n",
    "def build_models(models,x,y,test_size=0.33,random_state=42):\n",
    "    # spliting train test data using train_test_split\n",
    "    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=test_size,random_state=random_state)\n",
    "    \n",
    "    # training models using BinaryRelevance of problem transform\n",
    "    for i in tqdm.tqdm(models,desc=\"Building Models\"):\n",
    "        start_time = timeit.default_timer()\n",
    "        \n",
    "        sys.stdout.write(\"\\n=======================================================================================\\n\")\n",
    "        sys.stdout.write(f\"Current Model in Progress: {i} \")\n",
    "        sys.stdout.write(\"\\n=======================================================================================\\n\")\n",
    "        \n",
    "        br_clf = BinaryRelevance(classifier=models[i][\"name\"],require_dense=[True,True])\n",
    "        print(\"Training: \",br_clf)\n",
    "        br_clf.fit(x_train,y_train)\n",
    "        \n",
    "        print(\"Testing: \")\n",
    "        predict_y = br_clf.predict(x_test)\n",
    "        \n",
    "        ham_loss = hamming_loss(y_test,predict_y)\n",
    "        sys.stdout.write(f\"\\n\\tHamming Loss  : {ham_loss}\")\n",
    "                \n",
    "        ac_score = accuracy_score(y_test,predict_y)\n",
    "        sys.stdout.write(f\"\\n\\tAccuracy Score: {ac_score}\")\n",
    "        \n",
    "        cl_report = classification_report(y_test,predict_y)\n",
    "        sys.stdout.write(f\"\\n{cl_report}\")\n",
    "        \n",
    "        end_time = timeit.default_timer()\n",
    "        sys.stdout.write(f\"Completed in [{end_time-start_time} sec.]\")\n",
    "        \n",
    "        models[i][\"trained\"] = br_clf\n",
    "        models[i][\"hamming_loss\"] = ham_loss\n",
    "        models[i][\"accuracy_score\"] = ac_score\n",
    "        models[i][\"classification_report\"] = cl_report\n",
    "        models[i][\"predict_y\"] = predict_y\n",
    "        models[i][\"time_taken\"] = end_time - start_time\n",
    "                      \n",
    "        sys.stdout.write(\"\\n=======================================================================================\\n\")\n",
    "    \n",
    "    models[\"x_train\"] = x_train\n",
    "    models[\"y_train\"] = y_train\n",
    "    models[\"x_test\"] = x_test\n",
    "    models[\"y_test\"] = y_test\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e55d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the list of models for classification purpose\n",
    "models = {\"GaussianNB\": {\"name\": GaussianNB()},\n",
    "          \"MultinomialNB\": {\"name\": MultinomialNB()},\n",
    "          \"Logistic Regression\": {\"name\": LogisticRegression()},\n",
    "          \"Random Forest Classifier\": {\"name\": RandomForestClassifier()},\n",
    "          \"Support Vector Classifier\": {\"name\": LinearSVC(max_iter = 3000)},\n",
    "          \"Ada Boost Classifier\": {\"name\": AdaBoostClassifier()},\n",
    "          \"K Nearest Neighbors Classifier\": {\"name\": KNeighborsClassifier()},\n",
    "          \"Decision Tree Classifier\": {\"name\": DecisionTreeClassifier()},\n",
    "          \"Bagging Classifier\": {\"name\": BaggingClassifier(base_estimator=LinearSVC())},\n",
    "         }\n",
    "\n",
    "# Taking one forth of the total data for training and testing purpose\n",
    "half = len(df)//4\n",
    "trained_models = build_models(models,X[:half,:],Y[:half,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a6e4b",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5098d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing Linear Support Vector Classifier model\n",
    "\n",
    "fmod_param = {'estimator__penalty' : ['l1', 'l2'],\n",
    "              'estimator__loss' : ['hinge', 'squared_hinge'],\n",
    "              'estimator__multi_class' : ['ovr', 'crammer_singer'],\n",
    "              'estimator__random_state' : [42, 72, 111]\n",
    "             }\n",
    "SVC = OneVsRestClassifier(LinearSVC())\n",
    "GSCV = GridSearchCV(SVC, fmod_param, cv=3)\n",
    "x_train,x_test,y_train,y_test = train_test_split(X[:half,:], Y[:half,:], test_size=0.30, random_state=42)\n",
    "GSCV.fit(x_train,y_train)\n",
    "GSCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b598d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Model = OneVsRestClassifier(LinearSVC(loss='hinge', multi_class='ovr', penalty='l2', random_state=42))\n",
    "Classifier = Final_Model.fit(x_train, y_train)\n",
    "fmod_pred = Final_Model.predict(x_test)\n",
    "fmod_acc = (accuracy_score(y_test, fmod_pred))*100\n",
    "print(\"Accuracy score for the Best Model is:\", fmod_acc)\n",
    "h_loss = hamming_loss(y_test,fmod_pred)*100\n",
    "print(\"Hamming loss for the Best Model is:\", h_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09135050",
   "metadata": {},
   "source": [
    "# AUC ROC Curve for Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60a102",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = y_test.shape[1]\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], fmod_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), fmod_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8) # used to change the output figure size\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    fpr[\"micro\"],\n",
    "    tpr[\"micro\"],\n",
    "    label=\"micro-average ROC curve (AUC = {0:0.2f})\".format(roc_auc[\"micro\"]),\n",
    "    color=\"deeppink\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    fpr[\"macro\"],\n",
    "    tpr[\"macro\"],\n",
    "    label=\"macro-average ROC curve (AUC = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "    color=\"navy\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(\n",
    "        fpr[i],\n",
    "        tpr[i],\n",
    "        color=color,\n",
    "        lw=2,\n",
    "        label=\"ROC curve of class {0} (AUC = {1:0.2f})\".format(i, roc_auc[i]),\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver operating characteristic (ROC) and Area under curve (AUC) for multiclass labels\\n\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2547736b",
   "metadata": {},
   "source": [
    "# Confusion Matrix for Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf5134",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion matrix:\\n\\n\", multilabel_confusion_matrix(y_test, fmod_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e5a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,8) # used to change the output figure size\n",
    "ax= plt.subplot()\n",
    "cm = confusion_matrix(np.asarray(y_test).argmax(axis=1), np.asarray(fmod_pred).argmax(axis=1))\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax);  # annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "\n",
    "# title, labels and ticks\n",
    "ax.set_title('Confusion Matrix for the Final Classification Model\\n'); \n",
    "ax.set_xlabel('Predicted labels'); ax.set_ylabel('True labels'); \n",
    "loc = plticker.MultipleLocator()\n",
    "ax.xaxis.set_major_locator(loc); ax.yaxis.set_major_locator(loc);\n",
    "ax.set_xticklabels(comments_labels); ax.set_yticklabels(comments_labels);\n",
    "plt.xticks(rotation=90); plt.yticks(rotation=0);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d673ad4",
   "metadata": {},
   "source": [
    "# Model Saving or Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351663ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the best model\n",
    "best_model = trained_models['Support Vector Classifier']['trained']\n",
    "\n",
    "# saving the best classification model\n",
    "joblib.dump(best_model,open('Malignant_comments_classifier.pkl','wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8d1719b",
   "metadata": {},
   "source": [
    "Preprocessing Pipeline for test dataframe\n",
    "The following preprocessing pipeline is required to perform model prediction:\n",
    "\n",
    "Use the test dataset\n",
    "Remove null values if any\n",
    "Drop column id\n",
    "Convert comment text to lower case and replace '\\n' with single space\n",
    "Keep only text data ie. a-z' and remove other data from comment text\n",
    "Remove stop words and punctuations\n",
    "Apply Stemming using SnowballStemmer\n",
    "Convert text to vectors using TfidfVectorizer\n",
    "Load saved or serialized best model\n",
    "Predict values and create a new CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null values\n",
    "if df_test.isnull().sum()[1] != 0:\n",
    "    df_test.dropna(inplace=True)\n",
    "    \n",
    "# Drop coulmn id\n",
    "df_test.drop(columns=['id'],inplace=True)\n",
    "\n",
    "# Convert comment text to lower case and replace '\\n' with single space\n",
    "df_test[\"comment_text\"] = df_test.comment_text.str.lower()\n",
    "df_test[\"comment_text\"] = df_test.comment_text.str.replace('\\n',' ')\n",
    "\n",
    "# Keep only text data i.e., a-z' and remove other data from comment text.\n",
    "df_test.comment_text = df_test.comment_text.apply(lambda x: ' '.join(regexp_tokenize(x,\"[a-z']+\")))\n",
    "\n",
    "# Remove stopwords\n",
    "df_test.comment_text = df_test.comment_text.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words).strip())\n",
    "\n",
    "# Remove punctuations\n",
    "df_test.comment_text = df_test.comment_text.str.replace(\"[^\\w\\d\\s]\",\"\")\n",
    "\n",
    "# Apply Stemming using SnowballStemmer\n",
    "df_test.comment_text = df_test.comment_text.apply(lambda x: ' '.join(snb_stem.stem(word) for word in word_tokenize(x)))\n",
    "\n",
    "print(df_test.info(memory_usage=\"deep\"))\n",
    "\n",
    "# Convert text to vectors using TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(analyzer = 'word', max_features=4000)\n",
    "test_features = tfidf.fit_transform(df_test.comment_text).toarray()\n",
    "\n",
    "# Load saved or serialized model and predict\n",
    "model_loaded = joblib.load('Malignant_comments_classifier.pkl')\n",
    "\n",
    "# Make predictions and view the results\n",
    "predict_test = model_loaded.predict(test_features)\n",
    "\n",
    "# Saving predicted values into a CSV file\n",
    "pd.DataFrame(predict_test.toarray()).to_csv('Predicted_test_output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d7ed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Predicted_test_output.csv')\n",
    "df1.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df1.rename({'0':'malignant', '1':'highly_malignant', '2':'rude', '3':'threat', '4':'abuse', '5':'loathe'}, \n",
    "           axis='columns', inplace=True)\n",
    "df2=df_test.copy()\n",
    "df = pd.concat([df2, df1], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb5533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('test_dataset_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
